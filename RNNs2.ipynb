{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDL_Assignments6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBdiltzKOUcb"
      },
      "source": [
        "Reference- Last assignment+https://www.tensorflow.org/tutorials/text/text_generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6BCAUTgC9mG"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIPUhfDtP7B4",
        "outputId": "4136aefa-630b-4528-e873-74f0fdba02fa"
      },
      "source": [
        "!python prepare_data2.py shk_input.txt skp \\\\n -m 500"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-03 23:01:38.387300: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Split input into 40000 sequences...\n",
            "Longest sequence is 65 characters. If this seems unreasonable, consider using the maxlen argument!\n",
            "Removing sequences longer than 500 characters...\n",
            "40000 sequences remaining.\n",
            "Longest remaining sequence has length 65.\n",
            "Removing length-0 sequences...\n",
            "40000 sequences remaining.\n",
            "Serialized 100 sequences...\n",
            "Serialized 200 sequences...\n",
            "Serialized 300 sequences...\n",
            "Serialized 400 sequences...\n",
            "Serialized 500 sequences...\n",
            "Serialized 600 sequences...\n",
            "Serialized 700 sequences...\n",
            "Serialized 800 sequences...\n",
            "Serialized 900 sequences...\n",
            "Serialized 1000 sequences...\n",
            "Serialized 1100 sequences...\n",
            "Serialized 1200 sequences...\n",
            "Serialized 1300 sequences...\n",
            "Serialized 1400 sequences...\n",
            "Serialized 1500 sequences...\n",
            "Serialized 1600 sequences...\n",
            "Serialized 1700 sequences...\n",
            "Serialized 1800 sequences...\n",
            "Serialized 1900 sequences...\n",
            "Serialized 2000 sequences...\n",
            "Serialized 2100 sequences...\n",
            "Serialized 2200 sequences...\n",
            "Serialized 2300 sequences...\n",
            "Serialized 2400 sequences...\n",
            "Serialized 2500 sequences...\n",
            "Serialized 2600 sequences...\n",
            "Serialized 2700 sequences...\n",
            "Serialized 2800 sequences...\n",
            "Serialized 2900 sequences...\n",
            "Serialized 3000 sequences...\n",
            "Serialized 3100 sequences...\n",
            "Serialized 3200 sequences...\n",
            "Serialized 3300 sequences...\n",
            "Serialized 3400 sequences...\n",
            "Serialized 3500 sequences...\n",
            "Serialized 3600 sequences...\n",
            "Serialized 3700 sequences...\n",
            "Serialized 3800 sequences...\n",
            "Serialized 3900 sequences...\n",
            "Serialized 4000 sequences...\n",
            "Serialized 4100 sequences...\n",
            "Serialized 4200 sequences...\n",
            "Serialized 4300 sequences...\n",
            "Serialized 4400 sequences...\n",
            "Serialized 4500 sequences...\n",
            "Serialized 4600 sequences...\n",
            "Serialized 4700 sequences...\n",
            "Serialized 4800 sequences...\n",
            "Serialized 4900 sequences...\n",
            "Serialized 5000 sequences...\n",
            "Serialized 5100 sequences...\n",
            "Serialized 5200 sequences...\n",
            "Serialized 5300 sequences...\n",
            "Serialized 5400 sequences...\n",
            "Serialized 5500 sequences...\n",
            "Serialized 5600 sequences...\n",
            "Serialized 5700 sequences...\n",
            "Serialized 5800 sequences...\n",
            "Serialized 5900 sequences...\n",
            "Serialized 6000 sequences...\n",
            "Serialized 6100 sequences...\n",
            "Serialized 6200 sequences...\n",
            "Serialized 6300 sequences...\n",
            "Serialized 6400 sequences...\n",
            "Serialized 6500 sequences...\n",
            "Serialized 6600 sequences...\n",
            "Serialized 6700 sequences...\n",
            "Serialized 6800 sequences...\n",
            "Serialized 6900 sequences...\n",
            "Serialized 7000 sequences...\n",
            "Serialized 7100 sequences...\n",
            "Serialized 7200 sequences...\n",
            "Serialized 7300 sequences...\n",
            "Serialized 7400 sequences...\n",
            "Serialized 7500 sequences...\n",
            "Serialized 7600 sequences...\n",
            "Serialized 7700 sequences...\n",
            "Serialized 7800 sequences...\n",
            "Serialized 7900 sequences...\n",
            "Serialized 8000 sequences...\n",
            "Serialized 8100 sequences...\n",
            "Serialized 8200 sequences...\n",
            "Serialized 8300 sequences...\n",
            "Serialized 8400 sequences...\n",
            "Serialized 8500 sequences...\n",
            "Serialized 8600 sequences...\n",
            "Serialized 8700 sequences...\n",
            "Serialized 8800 sequences...\n",
            "Serialized 8900 sequences...\n",
            "Serialized 9000 sequences...\n",
            "Serialized 9100 sequences...\n",
            "Serialized 9200 sequences...\n",
            "Serialized 9300 sequences...\n",
            "Serialized 9400 sequences...\n",
            "Serialized 9500 sequences...\n",
            "Serialized 9600 sequences...\n",
            "Serialized 9700 sequences...\n",
            "Serialized 9800 sequences...\n",
            "Serialized 9900 sequences...\n",
            "Serialized 10000 sequences...\n",
            "Serialized 10100 sequences...\n",
            "Serialized 10200 sequences...\n",
            "Serialized 10300 sequences...\n",
            "Serialized 10400 sequences...\n",
            "Serialized 10500 sequences...\n",
            "Serialized 10600 sequences...\n",
            "Serialized 10700 sequences...\n",
            "Serialized 10800 sequences...\n",
            "Serialized 10900 sequences...\n",
            "Serialized 11000 sequences...\n",
            "Serialized 11100 sequences...\n",
            "Serialized 11200 sequences...\n",
            "Serialized 11300 sequences...\n",
            "Serialized 11400 sequences...\n",
            "Serialized 11500 sequences...\n",
            "Serialized 11600 sequences...\n",
            "Serialized 11700 sequences...\n",
            "Serialized 11800 sequences...\n",
            "Serialized 11900 sequences...\n",
            "Serialized 12000 sequences...\n",
            "Serialized 12100 sequences...\n",
            "Serialized 12200 sequences...\n",
            "Serialized 12300 sequences...\n",
            "Serialized 12400 sequences...\n",
            "Serialized 12500 sequences...\n",
            "Serialized 12600 sequences...\n",
            "Serialized 12700 sequences...\n",
            "Serialized 12800 sequences...\n",
            "Serialized 12900 sequences...\n",
            "Serialized 13000 sequences...\n",
            "Serialized 13100 sequences...\n",
            "Serialized 13200 sequences...\n",
            "Serialized 13300 sequences...\n",
            "Serialized 13400 sequences...\n",
            "Serialized 13500 sequences...\n",
            "Serialized 13600 sequences...\n",
            "Serialized 13700 sequences...\n",
            "Serialized 13800 sequences...\n",
            "Serialized 13900 sequences...\n",
            "Serialized 14000 sequences...\n",
            "Serialized 14100 sequences...\n",
            "Serialized 14200 sequences...\n",
            "Serialized 14300 sequences...\n",
            "Serialized 14400 sequences...\n",
            "Serialized 14500 sequences...\n",
            "Serialized 14600 sequences...\n",
            "Serialized 14700 sequences...\n",
            "Serialized 14800 sequences...\n",
            "Serialized 14900 sequences...\n",
            "Serialized 15000 sequences...\n",
            "Serialized 15100 sequences...\n",
            "Serialized 15200 sequences...\n",
            "Serialized 15300 sequences...\n",
            "Serialized 15400 sequences...\n",
            "Serialized 15500 sequences...\n",
            "Serialized 15600 sequences...\n",
            "Serialized 15700 sequences...\n",
            "Serialized 15800 sequences...\n",
            "Serialized 15900 sequences...\n",
            "Serialized 16000 sequences...\n",
            "Serialized 16100 sequences...\n",
            "Serialized 16200 sequences...\n",
            "Serialized 16300 sequences...\n",
            "Serialized 16400 sequences...\n",
            "Serialized 16500 sequences...\n",
            "Serialized 16600 sequences...\n",
            "Serialized 16700 sequences...\n",
            "Serialized 16800 sequences...\n",
            "Serialized 16900 sequences...\n",
            "Serialized 17000 sequences...\n",
            "Serialized 17100 sequences...\n",
            "Serialized 17200 sequences...\n",
            "Serialized 17300 sequences...\n",
            "Serialized 17400 sequences...\n",
            "Serialized 17500 sequences...\n",
            "Serialized 17600 sequences...\n",
            "Serialized 17700 sequences...\n",
            "Serialized 17800 sequences...\n",
            "Serialized 17900 sequences...\n",
            "Serialized 18000 sequences...\n",
            "Serialized 18100 sequences...\n",
            "Serialized 18200 sequences...\n",
            "Serialized 18300 sequences...\n",
            "Serialized 18400 sequences...\n",
            "Serialized 18500 sequences...\n",
            "Serialized 18600 sequences...\n",
            "Serialized 18700 sequences...\n",
            "Serialized 18800 sequences...\n",
            "Serialized 18900 sequences...\n",
            "Serialized 19000 sequences...\n",
            "Serialized 19100 sequences...\n",
            "Serialized 19200 sequences...\n",
            "Serialized 19300 sequences...\n",
            "Serialized 19400 sequences...\n",
            "Serialized 19500 sequences...\n",
            "Serialized 19600 sequences...\n",
            "Serialized 19700 sequences...\n",
            "Serialized 19800 sequences...\n",
            "Serialized 19900 sequences...\n",
            "Serialized 20000 sequences...\n",
            "Serialized 20100 sequences...\n",
            "Serialized 20200 sequences...\n",
            "Serialized 20300 sequences...\n",
            "Serialized 20400 sequences...\n",
            "Serialized 20500 sequences...\n",
            "Serialized 20600 sequences...\n",
            "Serialized 20700 sequences...\n",
            "Serialized 20800 sequences...\n",
            "Serialized 20900 sequences...\n",
            "Serialized 21000 sequences...\n",
            "Serialized 21100 sequences...\n",
            "Serialized 21200 sequences...\n",
            "Serialized 21300 sequences...\n",
            "Serialized 21400 sequences...\n",
            "Serialized 21500 sequences...\n",
            "Serialized 21600 sequences...\n",
            "Serialized 21700 sequences...\n",
            "Serialized 21800 sequences...\n",
            "Serialized 21900 sequences...\n",
            "Serialized 22000 sequences...\n",
            "Serialized 22100 sequences...\n",
            "Serialized 22200 sequences...\n",
            "Serialized 22300 sequences...\n",
            "Serialized 22400 sequences...\n",
            "Serialized 22500 sequences...\n",
            "Serialized 22600 sequences...\n",
            "Serialized 22700 sequences...\n",
            "Serialized 22800 sequences...\n",
            "Serialized 22900 sequences...\n",
            "Serialized 23000 sequences...\n",
            "Serialized 23100 sequences...\n",
            "Serialized 23200 sequences...\n",
            "Serialized 23300 sequences...\n",
            "Serialized 23400 sequences...\n",
            "Serialized 23500 sequences...\n",
            "Serialized 23600 sequences...\n",
            "Serialized 23700 sequences...\n",
            "Serialized 23800 sequences...\n",
            "Serialized 23900 sequences...\n",
            "Serialized 24000 sequences...\n",
            "Serialized 24100 sequences...\n",
            "Serialized 24200 sequences...\n",
            "Serialized 24300 sequences...\n",
            "Serialized 24400 sequences...\n",
            "Serialized 24500 sequences...\n",
            "Serialized 24600 sequences...\n",
            "Serialized 24700 sequences...\n",
            "Serialized 24800 sequences...\n",
            "Serialized 24900 sequences...\n",
            "Serialized 25000 sequences...\n",
            "Serialized 25100 sequences...\n",
            "Serialized 25200 sequences...\n",
            "Serialized 25300 sequences...\n",
            "Serialized 25400 sequences...\n",
            "Serialized 25500 sequences...\n",
            "Serialized 25600 sequences...\n",
            "Serialized 25700 sequences...\n",
            "Serialized 25800 sequences...\n",
            "Serialized 25900 sequences...\n",
            "Serialized 26000 sequences...\n",
            "Serialized 26100 sequences...\n",
            "Serialized 26200 sequences...\n",
            "Serialized 26300 sequences...\n",
            "Serialized 26400 sequences...\n",
            "Serialized 26500 sequences...\n",
            "Serialized 26600 sequences...\n",
            "Serialized 26700 sequences...\n",
            "Serialized 26800 sequences...\n",
            "Serialized 26900 sequences...\n",
            "Serialized 27000 sequences...\n",
            "Serialized 27100 sequences...\n",
            "Serialized 27200 sequences...\n",
            "Serialized 27300 sequences...\n",
            "Serialized 27400 sequences...\n",
            "Serialized 27500 sequences...\n",
            "Serialized 27600 sequences...\n",
            "Serialized 27700 sequences...\n",
            "Serialized 27800 sequences...\n",
            "Serialized 27900 sequences...\n",
            "Serialized 28000 sequences...\n",
            "Serialized 28100 sequences...\n",
            "Serialized 28200 sequences...\n",
            "Serialized 28300 sequences...\n",
            "Serialized 28400 sequences...\n",
            "Serialized 28500 sequences...\n",
            "Serialized 28600 sequences...\n",
            "Serialized 28700 sequences...\n",
            "Serialized 28800 sequences...\n",
            "Serialized 28900 sequences...\n",
            "Serialized 29000 sequences...\n",
            "Serialized 29100 sequences...\n",
            "Serialized 29200 sequences...\n",
            "Serialized 29300 sequences...\n",
            "Serialized 29400 sequences...\n",
            "Serialized 29500 sequences...\n",
            "Serialized 29600 sequences...\n",
            "Serialized 29700 sequences...\n",
            "Serialized 29800 sequences...\n",
            "Serialized 29900 sequences...\n",
            "Serialized 30000 sequences...\n",
            "Serialized 30100 sequences...\n",
            "Serialized 30200 sequences...\n",
            "Serialized 30300 sequences...\n",
            "Serialized 30400 sequences...\n",
            "Serialized 30500 sequences...\n",
            "Serialized 30600 sequences...\n",
            "Serialized 30700 sequences...\n",
            "Serialized 30800 sequences...\n",
            "Serialized 30900 sequences...\n",
            "Serialized 31000 sequences...\n",
            "Serialized 31100 sequences...\n",
            "Serialized 31200 sequences...\n",
            "Serialized 31300 sequences...\n",
            "Serialized 31400 sequences...\n",
            "Serialized 31500 sequences...\n",
            "Serialized 31600 sequences...\n",
            "Serialized 31700 sequences...\n",
            "Serialized 31800 sequences...\n",
            "Serialized 31900 sequences...\n",
            "Serialized 32000 sequences...\n",
            "Serialized 32100 sequences...\n",
            "Serialized 32200 sequences...\n",
            "Serialized 32300 sequences...\n",
            "Serialized 32400 sequences...\n",
            "Serialized 32500 sequences...\n",
            "Serialized 32600 sequences...\n",
            "Serialized 32700 sequences...\n",
            "Serialized 32800 sequences...\n",
            "Serialized 32900 sequences...\n",
            "Serialized 33000 sequences...\n",
            "Serialized 33100 sequences...\n",
            "Serialized 33200 sequences...\n",
            "Serialized 33300 sequences...\n",
            "Serialized 33400 sequences...\n",
            "Serialized 33500 sequences...\n",
            "Serialized 33600 sequences...\n",
            "Serialized 33700 sequences...\n",
            "Serialized 33800 sequences...\n",
            "Serialized 33900 sequences...\n",
            "Serialized 34000 sequences...\n",
            "Serialized 34100 sequences...\n",
            "Serialized 34200 sequences...\n",
            "Serialized 34300 sequences...\n",
            "Serialized 34400 sequences...\n",
            "Serialized 34500 sequences...\n",
            "Serialized 34600 sequences...\n",
            "Serialized 34700 sequences...\n",
            "Serialized 34800 sequences...\n",
            "Serialized 34900 sequences...\n",
            "Serialized 35000 sequences...\n",
            "Serialized 35100 sequences...\n",
            "Serialized 35200 sequences...\n",
            "Serialized 35300 sequences...\n",
            "Serialized 35400 sequences...\n",
            "Serialized 35500 sequences...\n",
            "Serialized 35600 sequences...\n",
            "Serialized 35700 sequences...\n",
            "Serialized 35800 sequences...\n",
            "Serialized 35900 sequences...\n",
            "Serialized 36000 sequences...\n",
            "Serialized 36100 sequences...\n",
            "Serialized 36200 sequences...\n",
            "Serialized 36300 sequences...\n",
            "Serialized 36400 sequences...\n",
            "Serialized 36500 sequences...\n",
            "Serialized 36600 sequences...\n",
            "Serialized 36700 sequences...\n",
            "Serialized 36800 sequences...\n",
            "Serialized 36900 sequences...\n",
            "Serialized 37000 sequences...\n",
            "Serialized 37100 sequences...\n",
            "Serialized 37200 sequences...\n",
            "Serialized 37300 sequences...\n",
            "Serialized 37400 sequences...\n",
            "Serialized 37500 sequences...\n",
            "Serialized 37600 sequences...\n",
            "Serialized 37700 sequences...\n",
            "Serialized 37800 sequences...\n",
            "Serialized 37900 sequences...\n",
            "Serialized 38000 sequences...\n",
            "Serialized 38100 sequences...\n",
            "Serialized 38200 sequences...\n",
            "Serialized 38300 sequences...\n",
            "Serialized 38400 sequences...\n",
            "Serialized 38500 sequences...\n",
            "Serialized 38600 sequences...\n",
            "Serialized 38700 sequences...\n",
            "Serialized 38800 sequences...\n",
            "Serialized 38900 sequences...\n",
            "Serialized 39000 sequences...\n",
            "Serialized 39100 sequences...\n",
            "Serialized 39200 sequences...\n",
            "Serialized 39300 sequences...\n",
            "Serialized 39400 sequences...\n",
            "Serialized 39500 sequences...\n",
            "Serialized 39600 sequences...\n",
            "Serialized 39700 sequences...\n",
            "Serialized 39800 sequences...\n",
            "Serialized 39900 sequences...\n",
            "Serialized 40000 sequences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMw8r1fMQVRw",
        "outputId": "91102bbf-9f05-40a8-9034-88b2b30cd412"
      },
      "source": [
        "#Getting the data\n",
        "from prepare_data2 import parse_seq\n",
        "import pickle\n",
        "\n",
        "# this is just a datasets of \"bytes\" (not understandable)\n",
        "data = tf.data.TFRecordDataset(\"skp.tfrecords\")\n",
        "\n",
        "# this maps a parser function that properly interprets the bytes over the dataset\n",
        "# (with fixed sequence length 200)\n",
        "# if you change the sequence length in preprocessing you also need to change it here\n",
        "data = data.map(parse_seq) #since changing seq lengths\n",
        "\n",
        "# a map from characters to indices\n",
        "vocab = pickle.load(open(\"skp_vocab\", mode=\"rb\"))\n",
        "vocab_size = len(vocab)\n",
        "# inverse mapping: indices to characters\n",
        "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
        "\n",
        "print(vocab)\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'F': 3, 'h': 4, 'O': 5, 's': 6, 'e': 7, 'c': 8, 'A': 9, 'N': 10, 'I': 11, 'W': 12, '3': 13, 'w': 14, 't': 15, 'H': 16, '!': 17, 'p': 18, '$': 19, ',': 20, 'U': 21, 'l': 22, 'M': 23, 'E': 24, 'Z': 25, 'z': 26, 'L': 27, 'G': 28, 'n': 29, 'q': 30, 'v': 31, '?': 32, 'T': 33, 'Y': 34, 'u': 35, '-': 36, ':': 37, 'J': 38, 'D': 39, ' ': 40, 'o': 41, 'K': 42, 'a': 43, 'B': 44, 'm': 45, 'R': 46, 'd': 47, '&': 48, 'X': 49, 'S': 50, 'g': 51, '\\n': 52, \"'\": 53, 'j': 54, 'Q': 55, 'b': 56, 'P': 57, 'V': 58, '.': 59, 'k': 60, 'f': 61, 'r': 62, 'i': 63, 'x': 64, ';': 65, 'y': 66, 'C': 67, '<PAD>': 0, '<S>': 1, '</S>': 2}\n",
            "68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10LqDHO315ri"
      },
      "source": [
        "#for current and expected time stamps\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "data = data.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYJypHXLTswB"
      },
      "source": [
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = data.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, drop_remainder=True) #to make the batch size of eqal since sequence is of variable lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTUeJjNwURbA"
      },
      "source": [
        "def build_model(vocab_size, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, 256, batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-yEluoPUXGG"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=68,\n",
        "    rnn_units=512,\n",
        "    batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7VNHsLIWPG2",
        "outputId": "8ffe7880-8b1f-45f4-c4b7-2abb72df01a0"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_27 (Embedding)     (128, None, 256)          17408     \n",
            "_________________________________________________________________\n",
            "lstm_27 (LSTM)               (128, None, 512)          1574912   \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (128, None, 68)           34884     \n",
            "=================================================================\n",
            "Total params: 1,627,204\n",
            "Trainable params: 1,627,204\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isleshGr7QS3",
        "outputId": "60118ed0-5b25-45df-e59f-64b7ee2bf02a"
      },
      "source": [
        "\n",
        "#running it\n",
        "epoch = 50\n",
        "opt = tf.keras.optimizers.Adam()\n",
        "for e in range(epoch):\n",
        "  for batch_num, (batch_data,y) in enumerate(data): #this is already batched of 128 batch size\n",
        "    total_loss = 0.0 #total loss over sequence \n",
        "    \n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      actual_chars = tf.TensorArray(tf.int64, size=BATCH_SIZE)\n",
        "      for char_pos, which_seq in enumerate(batch_data): #goign over the sequence in each batch\n",
        "            actual_chars = actual_chars.write(char_pos, tf.math.count_nonzero(which_seq))\n",
        "      mask = tf.sequence_mask(actual_chars.stack(), dtype=tf.float32)\n",
        "      logits = model(batch_data)\n",
        "      loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
        "      loss = loss * mask\n",
        "        \n",
        "    grads = tape.gradient(loss, model.trainable_variables) #gradients against the parametes/weights and biasas\n",
        "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    total_loss = tf.reduce_sum(loss)/float(tf.reduce_sum(actual_chars.stack())) # this is total loss over the whole batch\n",
        "    if batch_num % 100 == 0:\n",
        "          print('Epoch {} Batch {} Loss {}'.format(e, batch_num, total_loss))\n",
        "    \n",
        "\n",
        "  model.reset_states()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Batch 0 Loss 4.220046043395996\n",
            "Epoch 0 Batch 100 Loss 2.468925714492798\n",
            "Epoch 0 Batch 200 Loss 2.22340726852417\n",
            "Epoch 0 Batch 300 Loss 1.9901728630065918\n",
            "Epoch 1 Batch 0 Loss 2.4844579696655273\n",
            "Epoch 1 Batch 100 Loss 1.8927117586135864\n",
            "Epoch 1 Batch 200 Loss 1.756066918373108\n",
            "Epoch 1 Batch 300 Loss 1.767006516456604\n",
            "Epoch 2 Batch 0 Loss 1.8924732208251953\n",
            "Epoch 2 Batch 100 Loss 1.6891064643859863\n",
            "Epoch 2 Batch 200 Loss 1.6457432508468628\n",
            "Epoch 2 Batch 300 Loss 1.621482253074646\n",
            "Epoch 3 Batch 0 Loss 1.6948469877243042\n",
            "Epoch 3 Batch 100 Loss 1.5173479318618774\n",
            "Epoch 3 Batch 200 Loss 1.5612457990646362\n",
            "Epoch 3 Batch 300 Loss 1.503048300743103\n",
            "Epoch 4 Batch 0 Loss 1.5808364152908325\n",
            "Epoch 4 Batch 100 Loss 1.4707497358322144\n",
            "Epoch 4 Batch 200 Loss 1.5193747282028198\n",
            "Epoch 4 Batch 300 Loss 1.4869189262390137\n",
            "Epoch 5 Batch 0 Loss 1.4968221187591553\n",
            "Epoch 5 Batch 100 Loss 1.3834106922149658\n",
            "Epoch 5 Batch 200 Loss 1.3893227577209473\n",
            "Epoch 5 Batch 300 Loss 1.455060362815857\n",
            "Epoch 6 Batch 0 Loss 1.4261481761932373\n",
            "Epoch 6 Batch 100 Loss 1.312259554862976\n",
            "Epoch 6 Batch 200 Loss 1.4284826517105103\n",
            "Epoch 6 Batch 300 Loss 1.3869609832763672\n",
            "Epoch 7 Batch 0 Loss 1.4425435066223145\n",
            "Epoch 7 Batch 100 Loss 1.3294792175292969\n",
            "Epoch 7 Batch 200 Loss 1.3966361284255981\n",
            "Epoch 7 Batch 300 Loss 1.3578273057937622\n",
            "Epoch 8 Batch 0 Loss 1.3576551675796509\n",
            "Epoch 8 Batch 100 Loss 1.3075484037399292\n",
            "Epoch 8 Batch 200 Loss 1.3706451654434204\n",
            "Epoch 8 Batch 300 Loss 1.3320636749267578\n",
            "Epoch 9 Batch 0 Loss 1.3583970069885254\n",
            "Epoch 9 Batch 100 Loss 1.3165183067321777\n",
            "Epoch 9 Batch 200 Loss 1.3385610580444336\n",
            "Epoch 9 Batch 300 Loss 1.3229402303695679\n",
            "Epoch 10 Batch 0 Loss 1.3319092988967896\n",
            "Epoch 10 Batch 100 Loss 1.3391865491867065\n",
            "Epoch 10 Batch 200 Loss 1.3132025003433228\n",
            "Epoch 10 Batch 300 Loss 1.2898480892181396\n",
            "Epoch 11 Batch 0 Loss 1.2957571744918823\n",
            "Epoch 11 Batch 100 Loss 1.2331583499908447\n",
            "Epoch 11 Batch 200 Loss 1.254852294921875\n",
            "Epoch 11 Batch 300 Loss 1.2897299528121948\n",
            "Epoch 12 Batch 0 Loss 1.2718911170959473\n",
            "Epoch 12 Batch 100 Loss 1.2479597330093384\n",
            "Epoch 12 Batch 200 Loss 1.2339848279953003\n",
            "Epoch 12 Batch 300 Loss 1.2131963968276978\n",
            "Epoch 13 Batch 0 Loss 1.2585192918777466\n",
            "Epoch 13 Batch 100 Loss 1.2375367879867554\n",
            "Epoch 13 Batch 200 Loss 1.2669737339019775\n",
            "Epoch 13 Batch 300 Loss 1.2679803371429443\n",
            "Epoch 14 Batch 0 Loss 1.2468326091766357\n",
            "Epoch 14 Batch 100 Loss 1.2443716526031494\n",
            "Epoch 14 Batch 200 Loss 1.2993539571762085\n",
            "Epoch 14 Batch 300 Loss 1.247305989265442\n",
            "Epoch 15 Batch 0 Loss 1.2452749013900757\n",
            "Epoch 15 Batch 100 Loss 1.2091553211212158\n",
            "Epoch 15 Batch 200 Loss 1.2078207731246948\n",
            "Epoch 15 Batch 300 Loss 1.2249913215637207\n",
            "Epoch 16 Batch 0 Loss 1.2299786806106567\n",
            "Epoch 16 Batch 100 Loss 1.1957899332046509\n",
            "Epoch 16 Batch 200 Loss 1.2389386892318726\n",
            "Epoch 16 Batch 300 Loss 1.2270517349243164\n",
            "Epoch 17 Batch 0 Loss 1.1819616556167603\n",
            "Epoch 17 Batch 100 Loss 1.1810002326965332\n",
            "Epoch 17 Batch 200 Loss 1.2281566858291626\n",
            "Epoch 17 Batch 300 Loss 1.2244069576263428\n",
            "Epoch 18 Batch 0 Loss 1.2087067365646362\n",
            "Epoch 18 Batch 100 Loss 1.2045462131500244\n",
            "Epoch 18 Batch 200 Loss 1.226006031036377\n",
            "Epoch 18 Batch 300 Loss 1.161594271659851\n",
            "Epoch 19 Batch 0 Loss 1.1882902383804321\n",
            "Epoch 19 Batch 100 Loss 1.185585856437683\n",
            "Epoch 19 Batch 200 Loss 1.2186909914016724\n",
            "Epoch 19 Batch 300 Loss 1.207169771194458\n",
            "Epoch 20 Batch 0 Loss 1.2023745775222778\n",
            "Epoch 20 Batch 100 Loss 1.170411229133606\n",
            "Epoch 20 Batch 200 Loss 1.1910585165023804\n",
            "Epoch 20 Batch 300 Loss 1.1831740140914917\n",
            "Epoch 21 Batch 0 Loss 1.1233060359954834\n",
            "Epoch 21 Batch 100 Loss 1.1175650358200073\n",
            "Epoch 21 Batch 200 Loss 1.1659659147262573\n",
            "Epoch 21 Batch 300 Loss 1.2135595083236694\n",
            "Epoch 22 Batch 0 Loss 1.146486759185791\n",
            "Epoch 22 Batch 100 Loss 1.112298846244812\n",
            "Epoch 22 Batch 200 Loss 1.1494613885879517\n",
            "Epoch 22 Batch 300 Loss 1.1501319408416748\n",
            "Epoch 23 Batch 0 Loss 1.1192702054977417\n",
            "Epoch 23 Batch 100 Loss 1.1390701532363892\n",
            "Epoch 23 Batch 200 Loss 1.1654058694839478\n",
            "Epoch 23 Batch 300 Loss 1.1500234603881836\n",
            "Epoch 24 Batch 0 Loss 1.1118392944335938\n",
            "Epoch 24 Batch 100 Loss 1.096554160118103\n",
            "Epoch 24 Batch 200 Loss 1.1434969902038574\n",
            "Epoch 24 Batch 300 Loss 1.213261604309082\n",
            "Epoch 25 Batch 0 Loss 1.1410202980041504\n",
            "Epoch 25 Batch 100 Loss 1.1007682085037231\n",
            "Epoch 25 Batch 200 Loss 1.1435513496398926\n",
            "Epoch 25 Batch 300 Loss 1.158362865447998\n",
            "Epoch 26 Batch 0 Loss 1.1117228269577026\n",
            "Epoch 26 Batch 100 Loss 1.085267424583435\n",
            "Epoch 26 Batch 200 Loss 1.0869017839431763\n",
            "Epoch 26 Batch 300 Loss 1.1118468046188354\n",
            "Epoch 27 Batch 0 Loss 1.1165038347244263\n",
            "Epoch 27 Batch 100 Loss 1.0457676649093628\n",
            "Epoch 27 Batch 200 Loss 1.1512136459350586\n",
            "Epoch 27 Batch 300 Loss 1.130130648612976\n",
            "Epoch 28 Batch 0 Loss 1.1028915643692017\n",
            "Epoch 28 Batch 100 Loss 1.0665949583053589\n",
            "Epoch 28 Batch 200 Loss 1.1222097873687744\n",
            "Epoch 28 Batch 300 Loss 1.0615520477294922\n",
            "Epoch 29 Batch 0 Loss 1.0770002603530884\n",
            "Epoch 29 Batch 100 Loss 1.0448366403579712\n",
            "Epoch 29 Batch 200 Loss 1.0877476930618286\n",
            "Epoch 29 Batch 300 Loss 1.118017554283142\n",
            "Epoch 30 Batch 0 Loss 1.0845834016799927\n",
            "Epoch 30 Batch 100 Loss 1.0638236999511719\n",
            "Epoch 30 Batch 200 Loss 1.0705821514129639\n",
            "Epoch 30 Batch 300 Loss 1.0703964233398438\n",
            "Epoch 31 Batch 0 Loss 1.0809416770935059\n",
            "Epoch 31 Batch 100 Loss 1.0305547714233398\n",
            "Epoch 31 Batch 200 Loss 1.0778156518936157\n",
            "Epoch 31 Batch 300 Loss 1.1020313501358032\n",
            "Epoch 32 Batch 0 Loss 1.0705715417861938\n",
            "Epoch 32 Batch 100 Loss 1.0220776796340942\n",
            "Epoch 32 Batch 200 Loss 1.0654064416885376\n",
            "Epoch 32 Batch 300 Loss 1.0889168977737427\n",
            "Epoch 33 Batch 0 Loss 1.059454321861267\n",
            "Epoch 33 Batch 100 Loss 1.0008529424667358\n",
            "Epoch 33 Batch 200 Loss 1.0808790922164917\n",
            "Epoch 33 Batch 300 Loss 1.0285733938217163\n",
            "Epoch 34 Batch 0 Loss 1.0781409740447998\n",
            "Epoch 34 Batch 100 Loss 1.0014437437057495\n",
            "Epoch 34 Batch 200 Loss 1.0390794277191162\n",
            "Epoch 34 Batch 300 Loss 1.053697109222412\n",
            "Epoch 35 Batch 0 Loss 0.9847211837768555\n",
            "Epoch 35 Batch 100 Loss 0.9623843431472778\n",
            "Epoch 35 Batch 200 Loss 1.0252418518066406\n",
            "Epoch 35 Batch 300 Loss 1.0609996318817139\n",
            "Epoch 36 Batch 0 Loss 1.086993932723999\n",
            "Epoch 36 Batch 100 Loss 0.9847503900527954\n",
            "Epoch 36 Batch 200 Loss 0.9972707629203796\n",
            "Epoch 36 Batch 300 Loss 1.044578194618225\n",
            "Epoch 37 Batch 0 Loss 0.9962812066078186\n",
            "Epoch 37 Batch 100 Loss 0.9675424098968506\n",
            "Epoch 37 Batch 200 Loss 0.9870415925979614\n",
            "Epoch 37 Batch 300 Loss 1.008720874786377\n",
            "Epoch 38 Batch 0 Loss 1.0061359405517578\n",
            "Epoch 38 Batch 100 Loss 0.9999545216560364\n",
            "Epoch 38 Batch 200 Loss 0.9789034724235535\n",
            "Epoch 38 Batch 300 Loss 0.9617210030555725\n",
            "Epoch 39 Batch 0 Loss 1.0150527954101562\n",
            "Epoch 39 Batch 100 Loss 0.9680646657943726\n",
            "Epoch 39 Batch 200 Loss 1.035597801208496\n",
            "Epoch 39 Batch 300 Loss 0.9952176809310913\n",
            "Epoch 40 Batch 0 Loss 0.983601450920105\n",
            "Epoch 40 Batch 100 Loss 0.9377687573432922\n",
            "Epoch 40 Batch 200 Loss 0.9648647308349609\n",
            "Epoch 40 Batch 300 Loss 1.0061185359954834\n",
            "Epoch 41 Batch 0 Loss 0.9890079498291016\n",
            "Epoch 41 Batch 100 Loss 0.9818090796470642\n",
            "Epoch 41 Batch 200 Loss 0.972590982913971\n",
            "Epoch 41 Batch 300 Loss 1.0087214708328247\n",
            "Epoch 42 Batch 0 Loss 1.004822850227356\n",
            "Epoch 42 Batch 100 Loss 0.9269571304321289\n",
            "Epoch 42 Batch 200 Loss 0.9623835682868958\n",
            "Epoch 42 Batch 300 Loss 0.9754230380058289\n",
            "Epoch 43 Batch 0 Loss 0.987328827381134\n",
            "Epoch 43 Batch 100 Loss 0.9391972422599792\n",
            "Epoch 43 Batch 200 Loss 0.9594073295593262\n",
            "Epoch 43 Batch 300 Loss 0.9683880805969238\n",
            "Epoch 44 Batch 0 Loss 0.9598184823989868\n",
            "Epoch 44 Batch 100 Loss 0.929860532283783\n",
            "Epoch 44 Batch 200 Loss 0.9298253059387207\n",
            "Epoch 44 Batch 300 Loss 0.9584897756576538\n",
            "Epoch 45 Batch 0 Loss 0.9684928059577942\n",
            "Epoch 45 Batch 100 Loss 0.8952093124389648\n",
            "Epoch 45 Batch 200 Loss 0.9319831132888794\n",
            "Epoch 45 Batch 300 Loss 0.9889364838600159\n",
            "Epoch 46 Batch 0 Loss 0.9464119672775269\n",
            "Epoch 46 Batch 100 Loss 0.9182532429695129\n",
            "Epoch 46 Batch 200 Loss 0.9476320743560791\n",
            "Epoch 46 Batch 300 Loss 0.9583538770675659\n",
            "Epoch 47 Batch 0 Loss 0.9268444776535034\n",
            "Epoch 47 Batch 100 Loss 0.8692357540130615\n",
            "Epoch 47 Batch 200 Loss 0.9066991806030273\n",
            "Epoch 47 Batch 300 Loss 0.9139330387115479\n",
            "Epoch 48 Batch 0 Loss 0.9124689102172852\n",
            "Epoch 48 Batch 100 Loss 0.8935779333114624\n",
            "Epoch 48 Batch 200 Loss 0.9433445930480957\n",
            "Epoch 48 Batch 300 Loss 0.8867594599723816\n",
            "Epoch 49 Batch 0 Loss 0.9349033832550049\n",
            "Epoch 49 Batch 100 Loss 0.9013720750808716\n",
            "Epoch 49 Batch 200 Loss 0.9044401049613953\n",
            "Epoch 49 Batch 300 Loss 0.9161722660064697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SbJn3gnt4xg0",
        "outputId": "55de7273-69ff-4d23-8550-86f42cf8a9c9"
      },
      "source": [
        "\"\"\"tf.saved_model.save(model, \"/content/drive/MyDrive/lstm_folder\")\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tf.saved_model.save(model, \"/content/drive/MyDrive/lstm_folder\")'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu8eYURuoUdP"
      },
      "source": [
        "model.save_weights('/content/drive/MyDrive/lstm_folder/my_model_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0zJbwtT_BvW",
        "outputId": "48188c7e-9071-443b-e747-a4c3faa5aeea"
      },
      "source": [
        "model_p = build_model(\n",
        "    vocab_size=68,\n",
        "    rnn_units=512,\n",
        "    batch_size=1)\n",
        "model_p.load_weights(\"/content/drive/MyDrive/lstm_folder/my_model_weights.h5\")\n",
        "\n",
        "model_p.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model_p.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_29 (Embedding)     (1, None, 256)            17408     \n",
            "_________________________________________________________________\n",
            "lstm_29 (LSTM)               (1, None, 512)            1574912   \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (1, None, 68)             34884     \n",
            "=================================================================\n",
            "Total params: 1,627,204\n",
            "Trainable params: 1,627,204\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnZlomN09Tq3"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3A4Sg_VyZNy",
        "outputId": "f668209d-ecbf-4b23-89cd-27256a4c3f0d"
      },
      "source": [
        "idx2char"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array({'F': 3, 'h': 4, 'O': 5, 's': 6, 'e': 7, 'c': 8, 'A': 9, 'N': 10, 'I': 11, 'W': 12, '3': 13, 'w': 14, 't': 15, 'H': 16, '!': 17, 'p': 18, '$': 19, ',': 20, 'U': 21, 'l': 22, 'M': 23, 'E': 24, 'Z': 25, 'z': 26, 'L': 27, 'G': 28, 'n': 29, 'q': 30, 'v': 31, '?': 32, 'T': 33, 'Y': 34, 'u': 35, '-': 36, ':': 37, 'J': 38, 'D': 39, ' ': 40, 'o': 41, 'K': 42, 'a': 43, 'B': 44, 'm': 45, 'R': 46, 'd': 47, '&': 48, 'X': 49, 'S': 50, 'g': 51, '\\n': 52, \"'\": 53, 'j': 54, 'Q': 55, 'b': 56, 'P': 57, 'V': 58, '.': 59, 'k': 60, 'f': 61, 'r': 62, 'i': 63, 'x': 64, ';': 65, 'y': 66, 'C': 67, '<PAD>': 0, '<S>': 1, '</S>': 2},\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8ruZPyx767q"
      },
      "source": [
        "def generate_text(model_p, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 1000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model_p.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model_p(input_eval)\n",
        "        \n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        \n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        \n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        #print(\"predicted_id\",predicted_id)\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        #print(\"input_eval\",input_eval)\n",
        "        #print(ind_to_ch[predicted_id])\n",
        "\n",
        "        text_generated.append(ind_to_ch[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp4oUEqt8BXQ",
        "outputId": "03f72ac6-be75-4041-9462-f1ccb59e473d"
      },
      "source": [
        "print(generate_text(model_p, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO:  I beseech you on, then both lost</S>ly</S>on!</S></S>oke</S>ge</S>!</S>el</S>.</S></S>well,--</S>essen,</S>velous,</S>able!</S></S></S>lough!</S>'</S></S>les,</S>eve</S></S>ign</S>s</S>:</S>ether.</S></S>ly a</S>t,</S></S>age!</S></S>il</S>!</S></S>ight,</S>absky,</S>!</S>eir up! or want your height</S>en</S>ing,</S></S>ign!</S></S>tituge.</S></S>'</S></S></S>ent,</S>and</S>ow</S>ly</S>el:</S></S>wage!</S></S>igh</S>!</S></S>il'd,</S>rait</S>er</S>.</S></S>wel</S></S></S></S>el</S>;</S></S>wed</S>!</S>curs,</S>cate to instruct her</S>ines</S>.</S></S>ings</S>ing</S>t</S>e!</S></S></S>ing,</S></S>lowsed</S>y</S>!</S>e</S>ge</S></S>ess,</S>'</S>!</S>ereth.</S></S>ere</S>le</S>ters</S>alled,</S>hatiest</S>ward.</S></S>y</S>t</S>ig</S>!</S></S>ote,</S></S>ace</S>ol!</S>but,</S>'</S></S>ignsting</S>t</S>Mentain!</S>-Notens.</S>,</S>ever'd.</S></S>kering, as four lovers,</S>herance.</S></S>y</S>s</S>ing</S>ing</S></S>ins!</S></S>e, but</S>fired</S>y,</S>'d!</S>ermorade.</S></S></S>ing,</S></S>lack, which, since, of wit;</S>old</S>ur</S>!</S></S>well</S></S></S>.</S></S></S>et</S>,</S>aged:</S></S>in,</S>able</S>ed!</S></S>woked</S>e.</S></S></S>it</S>ed</S>.</S></S>bouting</S>t</S>es</S> me</S>ins.</S></S></S>ing,</S></S>look,</S>a's!</S></S>other:</S>er.</S></S>ow</S></S>new</S>y,</S></S>lough!</S></S>oked</S>quent;</S>o</S>d,</S></S>lough</S>ed,</S>ablo!</S>'</S> coaved</S>y!</S></S>y</S>s</S>y</S>f</S>il</S></S>ed</S></S></S>s</S></S>cy.</S></S>o</S>ewherate;</S></S>it</S>er</S>in</S></S>by.</S></S></S>llo!</S></S>ive</S>!</S>ere</S>le</S>ge</S>t</S>e!</S></S>il!</S>erget means thee</S>--</S>ope,</S>t</S>igh!</S></S>lo,</S>ed.</S></S>les</S>;</S></S>less,</S></S>lest,</S></S>y</S>age!</S></S>y</S>eld.</S></S></S>-thistes; I never</S></S>ign,</S>and</S>ateld</S>y.</S></S></S>t?</S>'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}