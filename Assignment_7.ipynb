{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1x60wP2Kcnf3y1tzPjMbcOH1TZsCrchip",
      "authorship_tag": "ABX9TyN+YM8VhGCATBb4AyCcs7uw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohadese-ghayoomi/Assignments-of-DL/blob/main/Assignment_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR4iFrFLwgIF"
      },
      "source": [
        "# **Importing the required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvpx8gtHbFAN"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kW-4juWwff3"
      },
      "source": [
        "# **Loading the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZwGYJUMCd0QE",
        "outputId": "ffb8387b-fe96-4492-e1a5-aa4bf1e1df3e"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPRG8loSd2U6"
      },
      "source": [
        "os.chdir('/content/drive/My Drive/Colab Notebooks')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKTthbzNbMDU"
      },
      "source": [
        "from google.colab import drive\r\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odRdewGwb05e",
        "outputId": "149cb114-bf69-403b-d6d4-9e80ec25d0dd"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L8-LfS341FW"
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z,اب پ ت س ج چ ح خ د ذ ر ز ژ س ش ص ض ط ظ ع غ ف ق ک گ ل م ن و ه ی, \".\", \"?\", \"!\", \",\")\n",
        "  #w = re.sub(r\"[^a-zA-Z?.!,¿ا ب پ ت س ج چ ح خ د ذ ر ز ژ س ش ص ض ط ظ ع غ ف ق ک گ ل م ن و ه ی]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewypq4st41H4",
        "outputId": "30d8a29d-ba9c-4f80-cdb4-bdf7c6cf8e51"
      },
      "source": [
        "en_sentence = u\"Get out!\"\n",
        "pes_sentence = u\"برو بیرون!\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(pes_sentence).encode('utf-8'))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> get out ! <end>\n",
            "b'<start> \\xd8\\xa8\\xd8\\xb1\\xd9\\x88 \\xd8\\xa8\\xdb\\x8c\\xd8\\xb1\\xd9\\x88\\xd9\\x86 ! <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8nyrSxz41J0"
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, persian]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:-1] ]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvACRGKkabYW",
        "outputId": "06d0c472-958e-42e7-d248-4325fb19b1f7"
      },
      "source": [
        "path_to_file = \"pes.txt\"\n",
        "en, pes = create_dataset(\"pes.txt\", 50)\n",
        "print(en)\n",
        "print(pes)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<start> who ? <end>', '<start> go on . <end>', '<start> smile . <end>', '<start> attack ! <end>', '<start> got it ! <end>', '<start> i know . <end>', '<start> listen . <end>', '<start> really ? <end>', '<start> really ? <end>', '<start> why me ? <end>', '<start> be cool . <end>', '<start> be cool . <end>', '<start> be cool . <end>', '<start> come in . <end>', '<start> come on ! <end>', '<start> get out ! <end>', '<start> get out ! <end>', '<start> go away ! <end>', '<start> go away ! <end>', '<start> help me ! <end>', '<start> help me ! <end>', '<start> hold it ! <end>', '<start> see you . <end>', '<start> see you . <end>', '<start> shut up ! <end>', '<start> so long . <end>', '<start> take it . <end>', '<start> tell me . <end>', '<start> welcome . <end>', '<start> get away ! <end>', '<start> grab him . <end>', '<start> hurry up . <end>', '<start> keep out ! <end>', '<start> speak up ! <end>', '<start> terrific ! <end>', '<start> we agree . <end>', '<start> what for ? <end>', '<start> aim . fire ! <end>', '<start> birds fly . <end>', '<start> calm down . <end>', \"<start> don't lie . <end>\", '<start> excuse me . <end>', '<start> fantastic ! <end>', \"<start> how's tom ? <end>\", '<start> i fainted . <end>', '<start> i will go . <end>', \"<start> i'm needy . <end>\", \"<start> i'm sorry . <end>\", \"<start> it's true . <end>\", '<start> let it go . <end>')\n",
            "('<start> چه کسی؟ <end>', '<start> ادامه بده ( ادامه دادن ) <end>', '<start> لبخند بزن . <end>', '<start> حمله ! <end>', '<start> گرفتم ! <end>', '<start> من می دانم . <end>', '<start> گوش کن . <end>', '<start> واقعا؟ <end>', '<start> جدا؟ <end>', '<start> چرا من؟ <end>', '<start> راحت باش . <end>', '<start> خونسرد باش . <end>', '<start> باحال باش . <end>', '<start> بیا داخل . بیا تو . <end>', '<start> بجنب ! <end>', '<start> برو بیرون ! <end>', '<start> پیاده شو ! <end>', '<start> برو بیرون ! <end>', '<start> گم شو ! <end>', '<start> به من کمک کن . <end>', '<start> کمکم کن . <end>', '<start> بایست ! <end>', '<start> به امید دیدار . <end>', '<start> میبینمت . <end>', '<start> خفه شو ! <end>', '<start> خداحافظ . <end>', '<start> بگیرش <end>', '<start> به من بگو . <end>', '<start> خوش امدید . <end>', '<start> دور شو ! <end>', '<start> دستگیرش کن . <end>', '<start> عجله کن ! <end>', '<start> برو بیرون ! <end>', '<start> حرفت را بزن ! <end>', '<start> فوق العاده <end>', '<start> ما به توافق رسیدیم . <end>', '<start> به چه دلیلی؟ <end>', '<start> هدف،اتش ! <end>', '<start> پرنده ها پرواز می کنند . <end>', '<start> ارام باش . <end>', '<start> دروغ نگو . <end>', '<start> ببخشید . <end>', '<start> خیلی با حال ! بسیار عالی ! <end>', '<start> تام حالش چطوره؟ <end>', '<start> من غش کردم . <end>', '<start> خواهم رفت . <end>', '<start> من نیازمندم . <end>', '<start> من متاسفم . <end>', '<start> درست است . <end>', '<start> ولش کن . <end>')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OKxumy841Tr"
      },
      "source": [
        "#This class allows to vectorize a text corpus, by turning each text into either a sequence of integers\r\n",
        "#(each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary\r\n",
        "def tokenize(lang):\r\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\r\n",
        "      filters='')\r\n",
        "#Trains the model for a fixed number of epochs (iterations on a dataset)\r\n",
        "  lang_tokenizer.fit_on_texts(lang)\r\n",
        "#Transforms each text in texts to a sequence of integers.\r\n",
        "#Each item in texts can also be a list, in which case we assume each item of that list to be a token.\r\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\r\n",
        "#Pads sequences to the same length\r\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\r\n",
        "                                                         padding='post') #pad after each sequence\r\n",
        "\r\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPxk5V-741Wf"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\r\n",
        "  # creating cleaned input, output pairs\r\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\r\n",
        "\r\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\r\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\r\n",
        "\r\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUd6Kpysy9ET"
      },
      "source": [
        "#**Limitation the size of the dataset to experiment faster**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_onAgUEM41ZE"
      },
      "source": [
        "# Try experimenting with the size of that dataset\r\n",
        "#num_examples of persian words = 2275\r\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, 2275)\r\n",
        "\r\n",
        "# Calculate max_length of the target tensors\r\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3UlU3-U1KGx",
        "outputId": "7b356c06-2abe-4489-e7d5-84abd2be60f1"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\r\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\r\n",
        "\r\n",
        "# Show length\r\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1820 1820 455 455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTzpNvOZ4ap2"
      },
      "source": [
        "def convert(lang, tensor):\r\n",
        "  for t in tensor:\r\n",
        "    if t!=0:\r\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i8CdPOa4ews",
        "outputId": "2b5c0ac7-4fcd-45dd-8800-fdbd0fb48624"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\r\n",
        "convert(inp_lang, input_tensor_train[2])\r\n",
        "print ()\r\n",
        "print (\"Target Language; index to word mapping\")\r\n",
        "convert(targ_lang, target_tensor_train[2])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "9 ----> او\n",
            "2897 ----> چیزهایی\n",
            "6 ----> را\n",
            "16 ----> با\n",
            "7 ----> من\n",
            "13 ----> در\n",
            "869 ----> میان\n",
            "5 ----> می\n",
            "759 ----> گذارد\n",
            "11 ----> که\n",
            "4 ----> به\n",
            "335 ----> هیچکس\n",
            "101 ----> دیگر\n",
            "17 ----> نمی\n",
            "460 ----> گوید\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "14 ----> he\n",
            "2215 ----> confided\n",
            "13 ----> in\n",
            "25 ----> me\n",
            "231 ----> things\n",
            "14 ----> he\n",
            "94 ----> would\n",
            "100 ----> tell\n",
            "76 ----> no\n",
            "65 ----> one\n",
            "537 ----> else\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO5uz11s5KKt"
      },
      "source": [
        "# **Create a tf.data dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o05tnCW5Jvv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}