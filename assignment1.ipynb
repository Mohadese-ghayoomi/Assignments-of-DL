# -*- coding: utf-8 -*-
"""Assignment1 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dm2sQTbR1cRJEsFclwBT5F4FzkNqGQzK

Multilayer perceptron for mnist **dataset**
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

class MNISTDataset:
    """'Bare minimum' class to wrap MNIST numpy arrays into a dataset."""
    def __init__(self, train_imgs, train_lbs, test_imgs, test_lbls, batch_size,
                 to01=True, shuffle=True, seed=None):
        """
        Use seed optionally to always get the same shuffling (-> reproducible
        results).
        """
        self.batch_size = batch_size
        self.train_data = train_imgs
        self.train_labels = train_lbs.astype(np.int32)
        self.test_data = test_imgs
        self.test_labels = test_lbls.astype(np.int32)

        if to01:
            # int in [0, 255] -> float in [0, 1]
            self.train_data = self.train_data.astype(np.float32) / 255
            self.test_data = self.test_data.astype(np.float32) / 255

        self.size = self.train_data.shape[0]

        if seed:
            np.random.seed(seed)
        if shuffle:
            self.shuffle_train()
        self.shuffle = shuffle
        self.current_pos = 0

    def next_batch(self):
        """Either gets the next batch, or optionally shuffles and starts a
        new epoch."""
        end_pos = self.current_pos + self.batch_size
        if end_pos < self.size:
            batch = (self.train_data[self.current_pos:end_pos],
                     self.train_labels[self.current_pos:end_pos])
            self.current_pos += self.batch_size
        else:
            # we return what's left (-> possibly smaller batch!) and prepare
            # the start of a new epoch
            batch = (self.train_data[self.current_pos:self.size],
                     self.train_labels[self.current_pos:self.size])
            if self.shuffle:
                self.shuffle_train()
            self.current_pos = 0
            print("Starting new epoch...")
        return batch

    def shuffle_train(self):
        shuffled_inds = np.arange(self.train_data.shape[0])
        np.random.shuffle(shuffled_inds)
        self.train_data = self.train_data[shuffled_inds]
        self.train_labels = self.train_labels[shuffled_inds]

        mnist = tf.keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

plt.imshow(train_images[0], cmap="Greys_r")

data = MNISTDataset(train_images.reshape([-1, 784]), train_labels, 
                    test_images.reshape([-1, 784]), test_labels,
                    batch_size=128)
print(type(data))

train_steps = 1000
learning_rate = 0.1


W = tf.Variable(tf.random.normal([784, 100], dtype=tf.float32))
b = tf.Variable(tf.random.normal([100], dtype=tf.float32))
w1 =  tf.Variable(tf.random.normal([100, 10], dtype=tf.float32))
b1 = tf.Variable(tf.random.normal([10], dtype=tf.float32))
accuracy = []
for step in range(train_steps):
    img_batch, lbl_batch = data.next_batch()
    with tf.GradientTape() as tape:
        logits = tf.matmul(img_batch, W) + b
       # print(logits.shape)
        layer = tf.nn.relu(tf.matmul(logits ,w1)+b1)
       # print(layer.shape)
        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(
            logits=layer, labels=lbl_batch))
        
    grads = tape.gradient(xent, [w1,b1])
    w1.assign_sub(learning_rate * grads[0])
    b1.assign_sub(learning_rate * grads[1])
    if not step % 100:
        preds = tf.argmax(layer, axis=1, output_type=tf.int32)
        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),
                             tf.float32))
        accuracy.append(acc)
        print("Loss: {} Accuracy: {}".format(xent, acc))

len(accuracy)

plt.plot(accuracy)